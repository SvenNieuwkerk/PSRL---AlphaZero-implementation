{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70de07a1-3203-48de-8185-fe5ca549aecc",
   "metadata": {},
   "source": [
    "# Imports, device and seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40f2261b-0766-4332-82cd-dc2b62407fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# --- Standard libs ---\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Any\n",
    "\n",
    "# --- Scientific stack ---\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# --- ACORL and RL ---\n",
    "from acorl.envs.seeker.seeker import SeekerEnv, SeekerEnvConfig\n",
    "\n",
    "# --- Own Code ---\n",
    "from MCTS import MCTSPlanner\n",
    "from network import SeekerAlphaZeroNet\n",
    "\n",
    "# --- Device ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "\n",
    "# --- Reproducibility ---\n",
    "def set_global_seeds(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "set_global_seeds(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8760f965-fb94-40f9-a40a-573edb1a81cd",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5eb04663-dbaa-4ab3-ad3c-a7667ce84392",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    # ========================\n",
    "    # Environment\n",
    "    # ========================\n",
    "    max_episode_steps: int = 200\n",
    "    # Maximum steps per real rollout episode\n",
    "\n",
    "\n",
    "    # ========================\n",
    "    # MCTS core\n",
    "    # ========================\n",
    "    num_simulations: int = 200\n",
    "    # Number of MCTS simulations per real environment step\n",
    "\n",
    "    cpuct: float = 1.5\n",
    "    # Exploration vs exploitation tradeoff in PUCT\n",
    "    # Higher -> more exploration guided by policy prior\n",
    "\n",
    "    gamma: float = 0.99\n",
    "    # Discount factor for return backup in MCTS\n",
    "\n",
    "    max_depth: int = 64\n",
    "    # Safety cap on tree depth during a simulation\n",
    "\n",
    "    temperature: float = 1.0\n",
    "    # Action sampling temperature at root\n",
    "    # >1.0 = more stochastic, 1.0 = proportional to visits, ~0 = greedy\n",
    "\n",
    "\n",
    "    # ========================\n",
    "    # Progressive Widening\n",
    "    # ========================\n",
    "    pw_k: float = 2.0\n",
    "    # Controls how many actions are allowed per node:\n",
    "    #   K_max = pw_k * N(s)^pw_alpha\n",
    "\n",
    "    pw_alpha: float = 0.5\n",
    "    # Growth rate of branching factor\n",
    "    # 0.5 is common; smaller = more conservative expansion\n",
    "\n",
    "\n",
    "    # ========================\n",
    "    # Action Sampling Improvements\n",
    "    # ========================\n",
    "\n",
    "    # --- Uniform warmstart ---\n",
    "    K_uniform_per_node: int = 5\n",
    "    # First K children per node are sampled uniformly in [-1,1]^2\n",
    "    # Set to 0 to disable\n",
    "\n",
    "    warmstart_iters: int = 0\n",
    "    # Number of *training iterations* during which ALL nodes use uniform sampling\n",
    "    # 0 disables global warmstart; use this if you want uniform sampling only early in training\n",
    "\n",
    "\n",
    "    # --- Novelty reject (hard deduplication) ---\n",
    "    novelty_eps: float = 0.1\n",
    "    # Minimum distance between actions to be considered \"new\"\n",
    "    # In [-1,1]^2, values around 0.05â€“0.15 are reasonable\n",
    "    # Set <=0 to disable\n",
    "\n",
    "    novelty_metric: str = \"linf\"\n",
    "    # Distance metric for novelty check:\n",
    "    # \"linf\" = max(|dx|, |dy|)  (good for box action spaces)\n",
    "    # \"l2\"   = Euclidean distance\n",
    "\n",
    "\n",
    "    # --- Diversity scoring (soft repulsion) ---\n",
    "    num_candidates: int = 16\n",
    "    # Number of candidate actions sampled before choosing the best\n",
    "    # <=1 disables diversity scoring\n",
    "\n",
    "    diversity_lambda: float = 1.0\n",
    "    # Strength of diversity penalty\n",
    "    # Higher -> stronger push away from already-sampled actions\n",
    "    # Set <=0 to disable\n",
    "\n",
    "    diversity_sigma: float = 0.25\n",
    "    # Length scale for diversity penalty\n",
    "    # Roughly: how far actions must be before they stop \"repelling\" each other\n",
    "\n",
    "    policy_beta: float = 1.0\n",
    "    # Weight of policy log-probability in candidate scoring\n",
    "    # Higher -> follow policy more closely\n",
    "    # Lower -> prioritize diversity more\n",
    "\n",
    "\n",
    "    # --- Resampling control ---\n",
    "    max_resample_attempts: int = 8\n",
    "    # How many times expansion may retry to find a novel action\n",
    "    # If all fail, expansion is declined and MCTS falls back to selection\n",
    "\n",
    "\n",
    "    # ========================\n",
    "    # Training\n",
    "    # ========================\n",
    "    batch_size: int = 128\n",
    "    learning_rate: float = 3e-4\n",
    "    weight_decay: float = 1e-4\n",
    "\n",
    "    train_steps_per_iter: int = 200\n",
    "    # Gradient updates per outer iteration\n",
    "\n",
    "    value_loss_weight: float = 1.0\n",
    "    policy_loss_weight: float = 1.0\n",
    "\n",
    "\n",
    "    # ========================\n",
    "    # Data collection\n",
    "    # ========================\n",
    "    episodes_per_iter: int = 10\n",
    "    # Number of real env episodes collected per training iteration\n",
    "\n",
    "    replay_buffer_capacity: int = 50_000\n",
    "\n",
    "\n",
    "    # ========================\n",
    "    # Logging / evaluation\n",
    "    # ========================\n",
    "    eval_every: int = 5\n",
    "    eval_episodes: int = 5\n",
    "\n",
    "\n",
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c4deec-de0b-4760-84f6-30507402327c",
   "metadata": {},
   "source": [
    "## Sanity test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbde1fad-f122-4484-b860-33233ef19456",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config(max_episode_steps=200,\n",
      "       num_simulations=200,\n",
      "       cpuct=1.5,\n",
      "       gamma=0.99,\n",
      "       max_depth=64,\n",
      "       temperature=1.0,\n",
      "       pw_k=2.0,\n",
      "       pw_alpha=0.5,\n",
      "       K_uniform_per_node=5,\n",
      "       warmstart_iters=0,\n",
      "       novelty_eps=0.1,\n",
      "       novelty_metric='linf',\n",
      "       num_candidates=16,\n",
      "       diversity_lambda=1.0,\n",
      "       diversity_sigma=0.25,\n",
      "       policy_beta=1.0,\n",
      "       max_resample_attempts=8,\n",
      "       batch_size=128,\n",
      "       learning_rate=0.0003,\n",
      "       weight_decay=0.0001,\n",
      "       train_steps_per_iter=200,\n",
      "       value_loss_weight=1.0,\n",
      "       policy_loss_weight=1.0,\n",
      "       episodes_per_iter=10,\n",
      "       replay_buffer_capacity=50000,\n",
      "       eval_every=5,\n",
      "       eval_episodes=5)\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2579f70f-487f-425b-a195-3e735208649a",
   "metadata": {},
   "source": [
    "# Create env_real, env_sim, dims (for network), and step_fn (for MCTSPlanner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0951bd8a-391e-48d2-b8f7-3df7a4c14d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs_dim: 7 action_dim: 2\n",
      "action_space: Box(-1.0, 1.0, (2,), float64)\n"
     ]
    }
   ],
   "source": [
    "# --- Env config ---\n",
    "env_config = SeekerEnvConfig(randomize=False, num_obstacles=1)\n",
    "\n",
    "# --- Real environment for rollouts / data collection ---\n",
    "env_real = SeekerEnv(**env_config.model_dump(exclude={\"id\"}))\n",
    "obs0, info0 = env_real.reset()\n",
    "\n",
    "obs_dim = env_real.observation_space.shape[0]\n",
    "action_dim = env_real.action_space.shape[0]\n",
    "\n",
    "print(\"obs_dim:\", obs_dim, \"action_dim:\", action_dim)\n",
    "print(\"action_space:\", env_real.action_space)\n",
    "\n",
    "# --- Simulation environment for MCTS step_fn ---\n",
    "env_sim = SeekerEnv(**env_config.model_dump(exclude={\"id\"}))\n",
    "\n",
    "\n",
    "def set_env_state_from_obs(sim_env: SeekerEnv, obs: np.ndarray):\n",
    "    \"\"\"\n",
    "    Overwrite SeekerEnv internal state to match the flat observation vector.\n",
    "\n",
    "    obs layout (from your old notebook):\n",
    "      [agent_x, agent_y, goal_x, goal_y, (obs_x, obs_y, obs_r)*N]\n",
    "    \"\"\"\n",
    "    obs = np.asarray(obs, dtype=sim_env._dtype)\n",
    "\n",
    "    # agent and goal\n",
    "    sim_env._agent_position = obs[0:2].copy()\n",
    "    sim_env._goal_position = obs[2:4].copy()\n",
    "\n",
    "    # obstacles\n",
    "    obstacles = obs[4:].reshape(-1, 3)\n",
    "    sim_env._obstacle_position = obstacles[:, 0:2].copy()\n",
    "    sim_env._obstacle_radius = obstacles[:, 2].copy()\n",
    "\n",
    "def step_fn(state: np.ndarray, action: np.ndarray):\n",
    "    \"\"\"\n",
    "    MCTS transition function: set env_sim to `state`, take `action`, return next_state/reward/done/info.\n",
    "    Returns: next_state, reward, done, info  (matching MCTSPlanner expectations)\n",
    "    \"\"\"\n",
    "    set_env_state_from_obs(env_sim, state)\n",
    "\n",
    "    action = np.asarray(action, dtype=env_sim._dtype)\n",
    "    next_obs, reward, terminated, truncated, info = env_sim.step(action)\n",
    "    done = bool(terminated or truncated)\n",
    "\n",
    "    return next_obs, float(reward), done, info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3fc4a2-6eb2-4d5f-a6b1-69d255066219",
   "metadata": {},
   "source": [
    "# Instantiate neural network and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0822f4f-0d85-45fa-97be-080dbc5b57d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mu: [[ 0.05101037 -0.15922213]]\n",
      "log_std: [[0.1659213 0.4990744]]\n",
      "v: 0.1396380364894867\n"
     ]
    }
   ],
   "source": [
    "# --- Network ---\n",
    "net = SeekerAlphaZeroNet(obs_dim=obs_dim, action_dim=action_dim).to(device)\n",
    "\n",
    "# Optional: print one forward pass sanity\n",
    "obs_t = torch.from_numpy(obs0).float().unsqueeze(0).to(device)\n",
    "with torch.no_grad():\n",
    "    mu_t, log_std_t, v_t = net(obs_t)\n",
    "\n",
    "print(\"mu:\", mu_t.cpu().numpy())\n",
    "print(\"log_std:\", log_std_t.cpu().numpy())\n",
    "print(\"v:\", v_t.item())\n",
    "\n",
    "# --- Optimizer (we'll use later) ---\n",
    "optimizer = optim.AdamW(net.parameters(), lr=cfg.learning_rate, weight_decay=cfg.weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe37236-6d23-4126-b98e-361792c51b95",
   "metadata": {},
   "source": [
    "# Instantiate MCTSPlanner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7047e6b0-329b-4d72-8659-6b9a61153410",
   "metadata": {},
   "outputs": [],
   "source": [
    "planner = MCTSPlanner(\n",
    "    net=net,\n",
    "    device=str(device),\n",
    "    step_fn=step_fn,\n",
    "    num_simulations=cfg.num_simulations,\n",
    "    cpuct=cfg.cpuct,\n",
    "    gamma=cfg.gamma,\n",
    "    pw_k=cfg.pw_k,\n",
    "    pw_alpha=cfg.pw_alpha,\n",
    "    max_depth=cfg.max_depth,\n",
    "    temperature=cfg.temperature,\n",
    "    rng=np.random.default_rng(SEED),\n",
    "    K_uniform_per_node=cfg.K_uniform_per_node,\n",
    "    warmstart_iters=cfg.warmstart_iters,\n",
    "    novelty_eps=cfg.novelty_eps,\n",
    "    novelty_metric=cfg.novelty_metric,\n",
    "    num_candidates=cfg.num_candidates,\n",
    "    diversity_lambda=cfg.diversity_lambda,\n",
    "    diversity_sigma=cfg.diversity_sigma,\n",
    "    policy_beta=cfg.policy_beta,\n",
    "    max_resample_attempts=cfg.max_resample_attempts,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69aa6a37-7e5b-45e2-a028-249e5c7e924c",
   "metadata": {},
   "source": [
    "# Smoke test: one MCTS search, inspect root, pick action, step env_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "211c00be-3295-4df4-8ae3-fdb83c0e234f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root visit count N: 200\n",
      "Root children K: 29\n",
      "[0] N_sa=   2  Q_sa=-1.8590  P_raw=7.835e-02  P=0.043  action=[-0.1131716 -0.5455226]\n",
      "[1] N_sa=   2  Q_sa=-1.4264  P_raw=7.958e-02  P=0.044  action=[-0.08216845  0.13748239]\n",
      "[2] N_sa=   2  Q_sa=-1.3088  P_raw=7.689e-02  P=0.042  action=[ 0.4447187  -0.07624554]\n",
      "[3] N_sa=   6  Q_sa=-1.0696  P_raw=7.059e-02  P=0.039  action=[0.51703906 0.43892592]\n",
      "[4] N_sa=   3  Q_sa=-1.2001  P_raw=6.342e-02  P=0.035  action=[-0.12980588  0.9847511 ]\n",
      "Chosen action: [1. 1.]\n",
      "Step result -> reward: 0.4142126045006318 done: False\n",
      "Obs delta L2: 1.4142135623730951\n"
     ]
    }
   ],
   "source": [
    "# Reset real env\n",
    "obs, info = env_real.reset()\n",
    "\n",
    "# Run one MCTS search from the current observation\n",
    "root = planner.search(obs)\n",
    "\n",
    "print(\"Root visit count N:\", root.N)\n",
    "print(\"Root children K:\", len(root.children))\n",
    "\n",
    "# Show a few children stats\n",
    "for i, ch in enumerate(root.children[:5]):\n",
    "    print(\n",
    "        f\"[{i}] N_sa={ch.N_sa:4d}  Q_sa={ch.Q_sa:+.4f}  \"\n",
    "        f\"P_raw={ch.P_sa_raw:.3e}  P={ch.P_sa:.3f}  action={ch.action}\"\n",
    "    )\n",
    "\n",
    "# Pick an action from MCTS policy (training=True samples from visit counts)\n",
    "action = planner.act(root, training=True)\n",
    "print(\"Chosen action:\", action)\n",
    "\n",
    "# Step the real environment once\n",
    "next_obs, reward, terminated, truncated, info = env_real.step(action)\n",
    "done = bool(terminated or truncated)\n",
    "\n",
    "print(\"Step result -> reward:\", reward, \"done:\", done)\n",
    "print(\"Obs delta L2:\", np.linalg.norm(next_obs - obs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "004c9ca5-92c2-4bb4-a2f6-7dba78b9264e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] N_sa=   2  Q_sa=-1.8590  P_raw=7.835e-02  P=0.043  action=[-0.1131716 -0.5455226]\n",
      "[1] N_sa=   2  Q_sa=-1.4264  P_raw=7.958e-02  P=0.044  action=[-0.08216845  0.13748239]\n",
      "[2] N_sa=   2  Q_sa=-1.3088  P_raw=7.689e-02  P=0.042  action=[ 0.4447187  -0.07624554]\n",
      "[3] N_sa=   6  Q_sa=-1.0696  P_raw=7.059e-02  P=0.039  action=[0.51703906 0.43892592]\n",
      "[4] N_sa=   3  Q_sa=-1.2001  P_raw=6.342e-02  P=0.035  action=[-0.12980588  0.9847511 ]\n",
      "[5] N_sa=   1  Q_sa=-1.4354  P_raw=7.030e-02  P=0.038  action=[ 0.2602494 -1.       ]\n",
      "[6] N_sa=   1  Q_sa=-1.6558  P_raw=5.476e-02  P=0.030  action=[-1.         -0.10073842]\n",
      "[7] N_sa=   1  Q_sa=-2.0700  P_raw=5.946e-02  P=0.033  action=[-0.6640612 -1.       ]\n",
      "[8] N_sa=   4  Q_sa=-1.2808  P_raw=5.875e-02  P=0.032  action=[ 1.        -0.2990462]\n",
      "[9] N_sa=  18  Q_sa=-1.0355  P_raw=5.998e-02  P=0.033  action=[0.46458045 1.        ]\n",
      "[10] N_sa=   2  Q_sa=-1.5904  P_raw=5.176e-02  P=0.028  action=[ 1. -1.]\n",
      "[11] N_sa=   2  Q_sa=-1.7778  P_raw=4.965e-02  P=0.027  action=[-1.          0.57921743]\n",
      "[12] N_sa=   1  Q_sa=-2.0691  P_raw=5.212e-02  P=0.028  action=[-1.        -0.6780454]\n",
      "[13] N_sa=  16  Q_sa=-0.9826  P_raw=5.433e-02  P=0.030  action=[1.       0.514771]\n",
      "[14] N_sa=   2  Q_sa=-1.6815  P_raw=7.250e-02  P=0.040  action=[-0.40246433  0.32182345]\n",
      "[15] N_sa=   2  Q_sa=-1.3160  P_raw=6.994e-02  P=0.038  action=[ 0.6237995 -0.5864497]\n",
      "[16] N_sa= 107  Q_sa=-0.7427  P_raw=4.622e-02  P=0.025  action=[1. 1.]\n",
      "[17] N_sa=   2  Q_sa=-1.7836  P_raw=7.184e-02  P=0.039  action=[-0.52988046 -0.01491621]\n",
      "[18] N_sa=   2  Q_sa=-1.1364  P_raw=5.499e-02  P=0.030  action=[-0.59166324  1.        ]\n",
      "[19] N_sa=   1  Q_sa=-1.6548  P_raw=7.109e-02  P=0.039  action=[-0.0597559 -1.       ]\n",
      "[20] N_sa=   2  Q_sa=-1.4961  P_raw=8.113e-02  P=0.044  action=[ 0.10016039 -0.25673246]\n",
      "[21] N_sa=   2  Q_sa=-1.2942  P_raw=7.030e-02  P=0.038  action=[-0.3086547   0.58359843]\n",
      "[22] N_sa=   2  Q_sa=-1.5132  P_raw=4.294e-02  P=0.023  action=[-1.  1.]\n",
      "[23] N_sa=   2  Q_sa=-1.5738  P_raw=6.048e-02  P=0.033  action=[ 0.7329641 -1.       ]\n",
      "[24] N_sa=   9  Q_sa=-1.0202  P_raw=6.305e-02  P=0.034  action=[0.22731893 1.        ]\n",
      "[25] N_sa=   1  Q_sa=-1.9106  P_raw=6.566e-02  P=0.036  action=[-0.43281603 -1.        ]\n",
      "[26] N_sa=   1  Q_sa=-1.4639  P_raw=7.353e-02  P=0.040  action=[-0.45626256 -0.37635967]\n",
      "[27] N_sa=   1  Q_sa=-2.3042  P_raw=4.809e-02  P=0.026  action=[-1. -1.]\n",
      "[28] N_sa=   3  Q_sa=-0.6283  P_raw=5.770e-02  P=0.032  action=[1.         0.19022098]\n",
      "unique rows: 29  /  29\n"
     ]
    }
   ],
   "source": [
    "# Show all children stats\n",
    "for i, ch in enumerate(root.children):\n",
    "    print(\n",
    "        f\"[{i}] N_sa={ch.N_sa:4d}  Q_sa={ch.Q_sa:+.4f}  \"\n",
    "        f\"P_raw={ch.P_sa_raw:.3e}  P={ch.P_sa:.3f}  action={ch.action}\"\n",
    "    )\n",
    "\n",
    "actions = np.stack([ch.action for ch in root.children], axis=0)\n",
    "print(\"unique rows:\", np.unique(actions, axis=0).shape[0], \" / \", actions.shape[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
