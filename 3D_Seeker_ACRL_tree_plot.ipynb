{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb34cba7-98d2-419d-9553-2566a94d62a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a7ce19f-99b6-4c8e-b234-22ab3135477b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from acorl.envs.seeker.seeker_exploration import SeekerExplorationEnvConfig\n",
    "import gymnasium as gym\n",
    "\n",
    "from acorl.env_wrapper.adaption_fn import ConditionalAdaptionEnvWrapper\n",
    "from acorl.acrl_algos.alpha_projection.mapping import alpha_projection_interface_fn\n",
    "from acorl.envs.constraints.seeker import SeekerInputSetPolytopeCalculator\n",
    "from rl_competition.competition.environment import create_exploration_seeker  # for env_config\n",
    "\n",
    "from plot_utils import plot_seeker_obs, decode_obs, plot_mcts_tree_xy_limited\n",
    "\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from network import SeekerAlphaZeroNet\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from MCTS_AC import MCTSPlanner_AC\n",
    "from utils import (\n",
    "    env_set_state,\n",
    ")\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4b13871-87f5-4ac1-b625-ba5dd97800c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Reproducibility ---\n",
    "def set_global_seeds(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "RNG_SEED = 42\n",
    "set_global_seeds(RNG_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b55adf56-51ce-4afc-810e-e82319c7b71a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# --- Device ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74b24915-d4d8-4ea5-8dfd-2e0878a84602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Config ===\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    # ========================\n",
    "    # Environment\n",
    "    # ========================\n",
    "    max_episode_steps: int = 300  # max steps in environment before cut off (goal not reached, obstacle not crashed into --> prevent forever stepping)\n",
    "\n",
    "    # ========================\n",
    "    # MCTS core\n",
    "    # ========================\n",
    "    num_simulations: int = 400    # Number of MCTS simulations per real environment step\n",
    "    cpuct: float = 4            # Exploration vs exploitation tradeoff in PUCT; Higher -> more exploration guided by policy prior\n",
    "    max_depth: int = 64           # Safety cap on tree depth during a simulation\n",
    "\n",
    "    # For root action selection / Action sampling temperature at root\n",
    "    # >1.0 = more stochastic, 1.0 = proportional to visits, ~0 = greedy\n",
    "    temperature: float = 1.0\n",
    "\n",
    "    # ========================\n",
    "    # Progressive Widening\n",
    "    # ========================\n",
    "    pw_k: float = 1.5\n",
    "    # Controls how many actions are allowed per node:\n",
    "    #   K_max = pw_k * N(s)^pw_alpha\n",
    "    pw_alpha: float = 0.5\n",
    "    # Growth rate of branching factor\n",
    "    # 0.5 is common; smaller = more conservative expansion\n",
    "\n",
    "    # ========================\n",
    "    # Action sampling (baseline, non-fancy, but no duplicates)\n",
    "    # ========================\n",
    "    # --- Uniform warmstart ---\n",
    "    # No uniform warmstart, no diversity scoring\n",
    "    K_uniform_per_node: int = 8\n",
    "    # First K children per node are sampled uniformly in [-1,1]^2\n",
    "    # Set to 0 to disable\n",
    "    warmstart_iters: int = 20\n",
    "    # Number of *training iterations* during which ALL nodes use uniform sampling\n",
    "    # 0 disables global warmstart; use this if you want uniform sampling only early in training\n",
    "\n",
    "    # --- Novelty reject (hard deduplication) ---\n",
    "    # Deduplicate actions (keep this ON to satisfy “no duplicate actions”)\n",
    "    novelty_eps: float = 1e-3      # small but > 0\n",
    "    # Minimum distance between actions to be considered \"new\"\n",
    "    # In [-1,1]^2, values around 0.05–0.15 are reasonable\n",
    "    # Set <=0 to disable\n",
    "    novelty_metric: str = \"l2\"\n",
    "    # Distance metric for novelty check:\n",
    "    # \"linf\" = max(|dx|, |dy|)  (good for box action spaces)\n",
    "    # \"l2\"   = Euclidean distance\n",
    "\n",
    "    # --- Diversity scoring (soft repulsion) ---\n",
    "    # Disable candidate scoring / diversity\n",
    "    num_candidates: int = 1\n",
    "    # Number of candidate actions sampled before choosing the best\n",
    "    # <=1 disables diversity scoring\n",
    "    diversity_lambda: float = 0.0\n",
    "    # Strength of diversity penalty\n",
    "    # Higher -> stronger push away from already-sampled actions\n",
    "    # Set <=0 to disable\n",
    "    diversity_sigma: float = 0.25  # unused\n",
    "    # Length scale for diversity penalty\n",
    "    # Roughly: how far actions must be before they stop \"repelling\" each other\n",
    "    policy_beta: float = 1.0       # unused\n",
    "    # Weight of policy log-probability in candidate scoring\n",
    "    # Higher -> follow policy more closely\n",
    "    # Lower -> prioritize diversity more\n",
    "\n",
    "    # --- Resampling control ---\n",
    "    max_resample_attempts: int = 16\n",
    "    # How many times expansion may retry to find a novel action\n",
    "    # If all fail, expansion is declined and MCTS falls back to selection\n",
    "    \n",
    "    # ========================\n",
    "    # Training\n",
    "    # ========================\n",
    "    batch_size: int = 8\n",
    "    learning_rate: float = 3e-4\n",
    "    weight_decay: float = 1e-4\n",
    "    train_steps_per_iter: int = 50    # Gradient updates per outer iteration\n",
    "\n",
    "    # (Only used by our baseline loss function)\n",
    "    value_loss_weight: float = 1.0\n",
    "    policy_loss_weight: float = 1.0  # applies to mu/log_std regression\n",
    "\n",
    "    # ========================\n",
    "    # Data collection\n",
    "    # ========================\n",
    "    collect_episodes_per_iter: int = 10     # Number of real env episodes collected per training iteration\n",
    "    replay_buffer_capacity: int = batch_size\n",
    "    gamma_mcts: float = 0.85     # Discount factor for return backup in MCTS\n",
    "    gamma_mc = 0.9\n",
    "\n",
    "    # ========================\n",
    "    # Logging / evaluation\n",
    "    # ========================\n",
    "    eval_every: int = 25\n",
    "    eval_episodes: int = 10   # use 10 fixed seeds for smoother eval curves\n",
    "\n",
    "\n",
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca23b971-a54b-4e88-bd62-f8954bc43448",
   "metadata": {},
   "outputs": [],
   "source": [
    "def action_towards_closest_obstacle(agent, obstacles, *, normalize=True):\n",
    "    \"\"\"\n",
    "    Compute an action vector pointing from the agent directly toward\n",
    "    the closest obstacle center.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    agent : array-like, shape (dim,)\n",
    "        Agent position.\n",
    "    obstacles : array-like, shape (N, dim+1)\n",
    "        Obstacle positions + radius. Only positions are used.\n",
    "    normalize : bool\n",
    "        If True, return a unit direction vector.\n",
    "        If False, return the raw displacement vector.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    action : np.ndarray, shape (dim,)\n",
    "        Action vector pointing toward the closest obstacle.\n",
    "    idx : int\n",
    "        Index of the closest obstacle.\n",
    "    dist : float\n",
    "        Distance to the closest obstacle center.\n",
    "    \"\"\"\n",
    "    agent = np.asarray(agent)\n",
    "    obstacles = np.asarray(obstacles)\n",
    "\n",
    "    centers = obstacles[:, :-1]  # drop radius\n",
    "    diffs = centers - agent      # vectors agent -> obstacle\n",
    "    dists = np.linalg.norm(diffs, axis=1)\n",
    "\n",
    "    idx = np.argmin(dists)\n",
    "    direction = diffs[idx]\n",
    "    dist = dists[idx]\n",
    "\n",
    "    if normalize:\n",
    "        if dist > 0:\n",
    "            direction = direction / dist\n",
    "        else:\n",
    "            direction = np.zeros_like(direction)\n",
    "\n",
    "    return direction, idx, dist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840ff4dd-7057-4cef-8176-296dd59a4c57",
   "metadata": {},
   "source": [
    "# ENVS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e95d3df-5cfb-45dd-8f2f-b3eb83915622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs_dim: 36 action_dim: 2\n",
      "action_space: Box(-1.0, 1.0, (2,), float64)\n"
     ]
    }
   ],
   "source": [
    "from rl_competition.competition.environment import create_exploration_seeker\n",
    "from acorl.envs.constraints.seeker import SeekerInputSetPolytopeCalculator\n",
    "from acorl.env_wrapper.adaption_fn import ConditionalAdaptionEnvWrapper\n",
    "from acorl.acrl_algos.alpha_projection.mapping import alpha_projection_interface_fn\n",
    "\n",
    "# --- Real environment for rollouts / data collection ---\n",
    "env_config = SeekerExplorationEnvConfig(\n",
    "        randomize=True,\n",
    "        num_obstacles=10,\n",
    "        dim=2,\n",
    "        log=False,\n",
    "    )\n",
    "env_real = gym.make(env_config.id, **env_config.model_dump(exclude={'id'}))\n",
    "env_real_AC = gym.make(env_config.id, **env_config.model_dump(exclude={'id'}))\n",
    "\n",
    "\n",
    "obs_dim = env_real.observation_space.shape[0]\n",
    "action_dim = env_real.action_space.shape[0]\n",
    "\n",
    "print(\"obs_dim:\", obs_dim, \"action_dim:\", action_dim)\n",
    "print(\"action_space:\", env_real.action_space)\n",
    "\n",
    "# --- Simulation environment for MCTS step_fn ---\n",
    "env_sim = gym.make(env_config.id, **env_config.model_dump(exclude={'id'}))\n",
    "env_sim = env_sim.unwrapped\n",
    "\n",
    "constraint_calculator = SeekerInputSetPolytopeCalculator(env_config=env_config)\n",
    "env_sim_AC = ConditionalAdaptionEnvWrapper(env_sim, \n",
    "                                        constraint_calculator.compute_relevant_input_set,\n",
    "                                        constraint_calculator.compute_fail_safe_input,\n",
    "                                        constraint_calculator.get_set_representation(),\n",
    "                                        alpha_projection_interface_fn)\n",
    "\n",
    "env_real_AC = ConditionalAdaptionEnvWrapper(env_real_AC, \n",
    "                                        constraint_calculator.compute_relevant_input_set,\n",
    "                                        constraint_calculator.compute_fail_safe_input,\n",
    "                                        constraint_calculator.get_set_representation(),\n",
    "                                        alpha_projection_interface_fn)\n",
    "\n",
    "def sync_conditional_adaption_wrapper(\n",
    "    env_wrapped,\n",
    "    obs,\n",
    "    *,\n",
    "    constraint_calculator,\n",
    "):\n",
    "    \"\"\"\n",
    "    Sync ConditionalAdaptionEnvWrapper cache after env_set_state().\n",
    "    \"\"\"\n",
    "    info = {\n",
    "        \"boundary_size\": float(getattr(env_wrapped.unwrapped, \"_size\", 10.0)),  # SeekerEnv uses _size for boundary :contentReference[oaicite:2]{index=2}\n",
    "    }\n",
    "\n",
    "    # Compute constraint info for THIS obs\n",
    "    info[\"relevant_input_set\"] = constraint_calculator.compute_relevant_input_set(obs, info)\n",
    "    info[\"fail_safe_input\"] = constraint_calculator.compute_fail_safe_input(obs, info)\n",
    "\n",
    "    # Optional but harmless\n",
    "    if hasattr(env_wrapped.unwrapped, \"_boundary_size\"):\n",
    "        info[\"boundary_size\"] = env_wrapped.unwrapped._boundary_size\n",
    "\n",
    "    env_wrapped._previous_obs = obs\n",
    "    env_wrapped._previous_info = info\n",
    "\n",
    "def step_fn(node, action):\n",
    "    \"\"\"\n",
    "    MCTS transition function: set env_sim to `state`, take `action`, return next_state/reward/done/info.\n",
    "    USES ACTION CONSTRAINED ENVIRONMENT\n",
    "    Returns: next_state, reward, done, info  (matching MCTSPlanner expectations)\n",
    "    \"\"\"\n",
    "    # 1) teleport base env\n",
    "    env_set_state(env_sim_AC, node, num_obstacles=env_config.num_obstacles)\n",
    "\n",
    "    # 2) sync wrapper cache\n",
    "    obs = np.asarray(node.state, dtype=env_sim.unwrapped._dtype)\n",
    "    sync_conditional_adaption_wrapper(\n",
    "        env_sim_AC,\n",
    "        obs,\n",
    "        constraint_calculator=constraint_calculator,\n",
    "    )\n",
    "\n",
    "\n",
    "    # 3) step the WRAPPED env\n",
    "    action = np.asarray(action, dtype=env_sim_AC.unwrapped._dtype)\n",
    "    next_obs, reward, terminated, truncated, info = env_sim_AC.step(action)\n",
    "\n",
    "    next_obs = np.array(next_obs, copy=True) #break reference to internal env buffer (??)\n",
    "    \n",
    "    done = bool(terminated or truncated)\n",
    "    next_coin_collected = bool(getattr(env_sim_AC.unwrapped, \"_coin_collected\", False))\n",
    "    \n",
    "    return next_obs, next_coin_collected, reward, done, info "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f6f5932-2ddf-4ae4-be9a-20909ccbfc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "obs0, info0 = env_real.reset(seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6c447a-4d8c-414b-9763-5b9fb35c6eb6",
   "metadata": {},
   "source": [
    "# NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7352b3a-80da-451e-8287-04bde14f63f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mu: [[0.3417295  0.20982334]]\n",
      "log_std: [[ 0.27098858 -0.38152447]]\n",
      "v: 0.8045704364776611\n"
     ]
    }
   ],
   "source": [
    "# --- Network ---\n",
    "net = SeekerAlphaZeroNet(obs_dim=obs_dim, action_dim=action_dim).to(device)\n",
    "\n",
    "# Optional: print one forward pass sanity\n",
    "obs_t = torch.from_numpy(obs0).float().unsqueeze(0).to(device)\n",
    "with torch.no_grad():\n",
    "    mu_t, log_std_t, v_t = net(obs_t)\n",
    "\n",
    "print(\"mu:\", mu_t.cpu().numpy())\n",
    "print(\"log_std:\", log_std_t.cpu().numpy())\n",
    "print(\"v:\", v_t.item())\n",
    "\n",
    "# --- Optimizer (we'll use later) ---\n",
    "optimizer = optim.AdamW(net.parameters(), lr=cfg.learning_rate, weight_decay=cfg.weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d857285f-7a4b-40e9-ae9c-6e866ba762d3",
   "metadata": {},
   "source": [
    "# PLANNER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3da8e94-2802-4a7e-bc9c-f49dc5016d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PLANNER === \n",
    "planner = MCTSPlanner_AC(\n",
    "    net=net,\n",
    "    device=str(device),\n",
    "    step_fn=step_fn,\n",
    "    num_simulations=cfg.num_simulations,\n",
    "    cpuct=cfg.cpuct,\n",
    "    gamma=cfg.gamma_mcts,\n",
    "    pw_k=cfg.pw_k,\n",
    "    pw_alpha=cfg.pw_alpha,\n",
    "    max_depth=cfg.max_depth,\n",
    "    temperature=cfg.temperature,\n",
    "    rng=np.random.default_rng(RNG_SEED),\n",
    "    \n",
    "    K_uniform_per_node=cfg.K_uniform_per_node,\n",
    "    warmstart_iters=cfg.warmstart_iters,\n",
    "    novelty_eps=cfg.novelty_eps,\n",
    "    novelty_metric=cfg.novelty_metric,\n",
    "    num_candidates=cfg.num_candidates,\n",
    "    diversity_lambda=cfg.diversity_lambda,\n",
    "    diversity_sigma=cfg.diversity_sigma,\n",
    "    policy_beta=cfg.policy_beta,\n",
    "    max_resample_attempts=cfg.max_resample_attempts,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cdf8d0a3-42be-4eed-b8a1-bb496791b885",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TimeLimit<OrderEnforcing<PassiveEnvChecker<SeekerExplorationEnv<acorl-envs/SeekerExplorationEnv>>>>>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa6da6c9-2bcf-439e-9b30-dac76fdf82ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "obs, info = env_real_AC.reset(seed=42)\n",
    "np.random.seed(42)\n",
    "obs, info = env_real.reset(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1880d439-7328-4489-875d-30f2c3de768d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_seeker_obs(obs, info, num_obstacles=10, env=env_real_AC, title=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4b65348-5403-483f-82eb-b2fa084441ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs:  [-2.26430661  9.99999056  4.17589095  1.77585272 -6.87962719 -6.88010959\n",
      "  0.41783754 -8.83832776  7.32352292  1.60190286  2.02230023  4.16145156\n",
      "  0.73762034 -9.58831011  9.39819704  1.11814671  6.64885282 -5.75321779\n",
      "  2.55870768 -6.36350066 -6.3319098   0.2221866  -3.91515514  0.49512863\n",
      "  1.81570856 -1.36109963 -4.1754172   0.65304704  2.23705789 -7.21012279\n",
      "  0.45900391 -4.15710703 -2.67276313  2.72099101  9.31264066  6.16794696]\n",
      "reward:  0\n",
      "terminated:  False\n",
      "truncated:  False\n",
      "info:  {'distance': 10.445697103260219, 'boundary_size': 10, 'require_fail_safe': False, 'relevant_input_set': <acorl.convexsets.hpolytope.HPolytope object at 0x000002A044B713D0>, 'fail_safe_input': array([None, None], dtype=object), 'adapted_action': array([-0.00602875,  0.88713305])}\n"
     ]
    }
   ],
   "source": [
    "action = [0, 1]\n",
    "obs, reward, terminated, truncated, info = env_real_AC.step(action)\n",
    "obs, reward, terminated, truncated, info = env_real_AC.step(action)\n",
    "print(\"obs: \", obs)\n",
    "print(\"reward: \", reward)\n",
    "print(\"terminated: \", terminated)\n",
    "print(\"truncated: \", truncated)\n",
    "print(\"info: \", info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2926a6a6-d758-4782-a440-70aa72dcbc73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs:  [-2.25827786 10.11285752  4.17589095  1.77585272 -6.87962719 -6.88010959\n",
      "  0.41783754 -8.83832776  7.32352292  1.60190286  2.02230023  4.16145156\n",
      "  0.73762034 -9.58831011  9.39819704  1.11814671  6.64885282 -5.75321779\n",
      "  2.55870768 -6.36350066 -6.3319098   0.2221866  -3.91515514  0.49512863\n",
      "  1.81570856 -1.36109963 -4.1754172   0.65304704  2.23705789 -7.21012279\n",
      "  0.45900391 -4.15710703 -2.67276313  2.72099101  9.31264066  6.16794696]\n",
      "reward:  -100.0\n",
      "terminated:  True\n",
      "truncated:  False\n",
      "info:  {'distance': 10.531105229326139, 'boundary_size': 10}\n"
     ]
    }
   ],
   "source": [
    "action = [0, 1]\n",
    "obs, reward, terminated, truncated, info = env_real.step(action)\n",
    "obs, reward, terminated, truncated, info = env_real.step(action)\n",
    "print(\"obs: \", obs)\n",
    "print(\"reward: \", reward)\n",
    "print(\"terminated: \", terminated)\n",
    "print(\"truncated: \", truncated)\n",
    "print(\"info: \", info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f6d30a9-4dbe-4a8b-b73d-585e258d3e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'distance': 7.616679783569992, 'boundary_size': 10}\n",
      "{'distance': 6.202536876075355, 'boundary_size': 10}\n",
      "{'distance': 4.788435702054005, 'boundary_size': 10}\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "obs, info = env_real.reset(seed=42)\n",
    "for i in range(3):\n",
    "    agent, goal, obstacles, coin, dim = decode_obs(obs, num_obstacles=10)\n",
    "    direction, idx, dist = action_towards_closest_obstacle(agent, obstacles, normalize=False)\n",
    "    action = np.clip(direction, -1.0, 1.0)\n",
    "    obs, reward, terminated, truncated, info = env_real.step(action)\n",
    "    print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe643a8e-317a-420d-8a2c-c10ccb1dab79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'distance': 4.0983404849984035, 'boundary_size': 10}\n"
     ]
    }
   ],
   "source": [
    "agent, goal, obstacles, coin, dim = decode_obs(obs, num_obstacles=10)\n",
    "direction, idx, dist = action_towards_closest_obstacle(agent, obstacles, normalize=False)\n",
    "action = 0.5*np.clip(direction, -1.0, 1.0)\n",
    "obs, reward, terminated, truncated, info = env_real.step(action)\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5f68643d-3501-4434-98db-3f3cecdca3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_seeker_obs(obs, info, num_obstacles=10, env=env_real, title=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "08005693-71c8-4a0d-aecf-1cb432d0ce54",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = planner.search(obs, coin_collected=bool(getattr(env_real.unwrapped, \"_coin_collected\", False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0acd67ec-351c-4e18-8356-b11a116dd712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: title={'center': 'MCTS tree near obstacle (2D)'}>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_mcts_tree_xy_limited(\n",
    "    root,\n",
    "    num_obstacles=10,\n",
    "    title=\"MCTS tree near obstacle (2D)\",\n",
    "    max_depth=1,\n",
    "    top_k_per_node=99,\n",
    "    L=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5e6bd737-d272-491f-9903-c5bebb1bf9e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 terminal children\n"
     ]
    }
   ],
   "source": [
    "unsafe = [ch for ch in root.children if ch.child_node.is_terminal]\n",
    "print(len(unsafe), \"terminal children\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bcd5d7-cff7-415c-956a-b9f29ef28dd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
