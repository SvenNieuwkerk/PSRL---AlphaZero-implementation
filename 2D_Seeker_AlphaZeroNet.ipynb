{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70de07a1-3203-48de-8185-fe5ca549aecc",
   "metadata": {},
   "source": [
    "# Imports, device and seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40f2261b-0766-4332-82cd-dc2b62407fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# --- Standard libs ---\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Any\n",
    "\n",
    "# --- Scientific stack ---\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- ACORL and RL ---\n",
    "from acorl.envs.seeker.seeker import SeekerEnv, SeekerEnvConfig\n",
    "\n",
    "# --- Own Code ---\n",
    "from MCTS import MCTSPlanner\n",
    "from network import SeekerAlphaZeroNet\n",
    "from utils import ReplayBufferHybrid, collect_one_episode_hybrid, train_step_mle, train_step_mcts_distill, train_step_hybrid, run_eval_episodes\n",
    "\n",
    "# --- Device ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "\n",
    "# --- Reproducibility ---\n",
    "def set_global_seeds(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "set_global_seeds(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8760f965-fb94-40f9-a40a-573edb1a81cd",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40daddd1-6007-44a9-b0ee-56ae61624abc",
   "metadata": {},
   "source": [
    "## baseline config\n",
    "### Comments are explaining what each parameter does in general, but also mixed with comments that explain behavious of this specific configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44435135-4d44-463e-83a4-af8fb39898bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    # ========================\n",
    "    # Environment\n",
    "    # ========================\n",
    "    max_episode_steps: int = 200\n",
    "\n",
    "    # ========================\n",
    "    # MCTS core\n",
    "    # ========================\n",
    "    num_simulations: int = 200    # Number of MCTS simulations per real environment step\n",
    "    cpuct: float = 1.5            # Exploration vs exploitation tradeoff in PUCT; Higher -> more exploration guided by policy prior\n",
    "    gamma: float = 0.99           # Discount factor for return backup in MCTS\n",
    "    max_depth: int = 64           # Safety cap on tree depth during a simulation\n",
    "\n",
    "    # For root action selection / Action sampling temperature at root\n",
    "    # >1.0 = more stochastic, 1.0 = proportional to visits, ~0 = greedy\n",
    "    temperature: float = 1.0\n",
    "\n",
    "    # ========================\n",
    "    # Progressive Widening\n",
    "    # ========================\n",
    "    pw_k: float = 2.0\n",
    "    # Controls how many actions are allowed per node:\n",
    "    #   K_max = pw_k * N(s)^pw_alpha\n",
    "    pw_alpha: float = 0.5\n",
    "    # Growth rate of branching factor\n",
    "    # 0.5 is common; smaller = more conservative expansion\n",
    "\n",
    "    # ========================\n",
    "    # Action sampling (baseline, non-fancy, but no duplicates)\n",
    "    # ========================\n",
    "    # --- Uniform warmstart ---\n",
    "    # No uniform warmstart, no diversity scoring\n",
    "    K_uniform_per_node: int = 0\n",
    "    # First K children per node are sampled uniformly in [-1,1]^2\n",
    "    # Set to 0 to disable\n",
    "    warmstart_iters: int = 0\n",
    "    # Number of *training iterations* during which ALL nodes use uniform sampling\n",
    "    # 0 disables global warmstart; use this if you want uniform sampling only early in training\n",
    "\n",
    "    # --- Novelty reject (hard deduplication) ---\n",
    "    # Deduplicate actions (keep this ON to satisfy “no duplicate actions”)\n",
    "    novelty_eps: float = 1e-3      # small but > 0\n",
    "    # Minimum distance between actions to be considered \"new\"\n",
    "    # In [-1,1]^2, values around 0.05–0.15 are reasonable\n",
    "    # Set <=0 to disable\n",
    "    novelty_metric: str = \"linf\"\n",
    "    # Distance metric for novelty check:\n",
    "    # \"linf\" = max(|dx|, |dy|)  (good for box action spaces)\n",
    "    # \"l2\"   = Euclidean distance\n",
    "\n",
    "    # --- Diversity scoring (soft repulsion) ---\n",
    "    # Disable candidate scoring / diversity\n",
    "    num_candidates: int = 1\n",
    "    # Number of candidate actions sampled before choosing the best\n",
    "    # <=1 disables diversity scoring\n",
    "    diversity_lambda: float = 0.0\n",
    "    # Strength of diversity penalty\n",
    "    # Higher -> stronger push away from already-sampled actions\n",
    "    # Set <=0 to disable\n",
    "    diversity_sigma: float = 0.25  # unused\n",
    "    # Length scale for diversity penalty\n",
    "    # Roughly: how far actions must be before they stop \"repelling\" each other\n",
    "    policy_beta: float = 1.0       # unused\n",
    "    # Weight of policy log-probability in candidate scoring\n",
    "    # Higher -> follow policy more closely\n",
    "    # Lower -> prioritize diversity more\n",
    "\n",
    "    # --- Resampling control ---\n",
    "    max_resample_attempts: int = 16\n",
    "    # How many times expansion may retry to find a novel action\n",
    "    # If all fail, expansion is declined and MCTS falls back to selection\n",
    "    \n",
    "    # ========================\n",
    "    # Training\n",
    "    # ========================\n",
    "    batch_size: int = 128\n",
    "    learning_rate: float = 3e-4\n",
    "    weight_decay: float = 1e-4\n",
    "    train_steps_per_iter: int = 200    # Gradient updates per outer iteration\n",
    "\n",
    "    # (Only used by our baseline loss function)\n",
    "    value_loss_weight: float = 1.0\n",
    "    policy_loss_weight: float = 1.0  # applies to mu/log_std regression\n",
    "\n",
    "    # ========================\n",
    "    # Data collection\n",
    "    # ========================\n",
    "    episodes_per_iter: int = 10     # Number of real env episodes collected per training iteration\n",
    "    replay_buffer_capacity: int = 50_000\n",
    "\n",
    "    # ========================\n",
    "    # Logging / evaluation\n",
    "    # ========================\n",
    "    eval_every: int = 5\n",
    "    eval_episodes: int = 10   # use 10 fixed seeds for smoother eval curves\n",
    "\n",
    "\n",
    "cfg = Config()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c4deec-de0b-4760-84f6-30507402327c",
   "metadata": {},
   "source": [
    "## Sanity test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbde1fad-f122-4484-b860-33233ef19456",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config(max_episode_steps=200,\n",
      "       num_simulations=200,\n",
      "       cpuct=1.5,\n",
      "       gamma=0.99,\n",
      "       max_depth=64,\n",
      "       temperature=1.0,\n",
      "       pw_k=2.0,\n",
      "       pw_alpha=0.5,\n",
      "       K_uniform_per_node=0,\n",
      "       warmstart_iters=0,\n",
      "       novelty_eps=0.001,\n",
      "       novelty_metric='linf',\n",
      "       num_candidates=1,\n",
      "       diversity_lambda=0.0,\n",
      "       diversity_sigma=0.25,\n",
      "       policy_beta=1.0,\n",
      "       max_resample_attempts=16,\n",
      "       batch_size=128,\n",
      "       learning_rate=0.0003,\n",
      "       weight_decay=0.0001,\n",
      "       train_steps_per_iter=200,\n",
      "       value_loss_weight=1.0,\n",
      "       policy_loss_weight=1.0,\n",
      "       episodes_per_iter=10,\n",
      "       replay_buffer_capacity=50000,\n",
      "       eval_every=5,\n",
      "       eval_episodes=10)\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2579f70f-487f-425b-a195-3e735208649a",
   "metadata": {},
   "source": [
    "# Create env_real, env_sim, dims (for network), and step_fn (for MCTSPlanner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0951bd8a-391e-48d2-b8f7-3df7a4c14d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs_dim: 7 action_dim: 2\n",
      "action_space: Box(-1.0, 1.0, (2,), float64)\n"
     ]
    }
   ],
   "source": [
    "EVAL_SEEDS = list(range(1000, 1000 + cfg.eval_episodes))  # fixed \"validation set\"\n",
    "\n",
    "# --- Env config ---\n",
    "env_config = SeekerEnvConfig(randomize=True, num_obstacles=1)\n",
    "\n",
    "# --- Real environment for rollouts / data collection ---\n",
    "env_real = SeekerEnv(**env_config.model_dump(exclude={\"id\"}))\n",
    "obs0, info0 = env_real.reset()\n",
    "\n",
    "obs_dim = env_real.observation_space.shape[0]\n",
    "action_dim = env_real.action_space.shape[0]\n",
    "\n",
    "print(\"obs_dim:\", obs_dim, \"action_dim:\", action_dim)\n",
    "print(\"action_space:\", env_real.action_space)\n",
    "\n",
    "# --- Simulation environment for MCTS step_fn ---\n",
    "env_sim = SeekerEnv(**env_config.model_dump(exclude={\"id\"}))\n",
    "\n",
    "# --- evaluation environment for evaluation during training ---\n",
    "env_eval = SeekerEnv(**env_config.model_dump(exclude={\"id\"}))\n",
    "\n",
    "\n",
    "def set_env_state_from_obs(sim_env: SeekerEnv, obs: np.ndarray):\n",
    "    \"\"\"\n",
    "    Overwrite SeekerEnv internal state to match the flat observation vector.\n",
    "\n",
    "    obs layout (from your old notebook):\n",
    "      [agent_x, agent_y, goal_x, goal_y, (obs_x, obs_y, obs_r)*N]\n",
    "    \"\"\"\n",
    "    obs = np.asarray(obs, dtype=sim_env._dtype)\n",
    "\n",
    "    # agent and goal\n",
    "    sim_env._agent_position = obs[0:2].copy()\n",
    "    sim_env._goal_position = obs[2:4].copy()\n",
    "\n",
    "    # obstacles\n",
    "    obstacles = obs[4:].reshape(-1, 3)\n",
    "    sim_env._obstacle_position = obstacles[:, 0:2].copy()\n",
    "    sim_env._obstacle_radius = obstacles[:, 2].copy()\n",
    "\n",
    "def step_fn(state: np.ndarray, action: np.ndarray):\n",
    "    \"\"\"\n",
    "    MCTS transition function: set env_sim to `state`, take `action`, return next_state/reward/done/info.\n",
    "    Returns: next_state, reward, done, info  (matching MCTSPlanner expectations)\n",
    "    \"\"\"\n",
    "    set_env_state_from_obs(env_sim, state)\n",
    "\n",
    "    action = np.asarray(action, dtype=env_sim._dtype)\n",
    "    next_obs, reward, terminated, truncated, info = env_sim.step(action)\n",
    "    done = bool(terminated or truncated)\n",
    "\n",
    "    return next_obs, float(reward), done, info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3fc4a2-6eb2-4d5f-a6b1-69d255066219",
   "metadata": {},
   "source": [
    "# Instantiate neural network and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0822f4f-0d85-45fa-97be-080dbc5b57d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mu: [[-0.26337135 -0.13172348]]\n",
      "log_std: [[ 0.45294833 -0.14332369]]\n",
      "v: 0.02973608300089836\n"
     ]
    }
   ],
   "source": [
    "# --- Network ---\n",
    "net = SeekerAlphaZeroNet(obs_dim=obs_dim, action_dim=action_dim).to(device)\n",
    "\n",
    "# Optional: print one forward pass sanity\n",
    "obs_t = torch.from_numpy(obs0).float().unsqueeze(0).to(device)\n",
    "with torch.no_grad():\n",
    "    mu_t, log_std_t, v_t = net(obs_t)\n",
    "\n",
    "print(\"mu:\", mu_t.cpu().numpy())\n",
    "print(\"log_std:\", log_std_t.cpu().numpy())\n",
    "print(\"v:\", v_t.item())\n",
    "\n",
    "# --- Optimizer (we'll use later) ---\n",
    "optimizer = optim.AdamW(net.parameters(), lr=cfg.learning_rate, weight_decay=cfg.weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe37236-6d23-4126-b98e-361792c51b95",
   "metadata": {},
   "source": [
    "# Instantiate MCTSPlanner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7047e6b0-329b-4d72-8659-6b9a61153410",
   "metadata": {},
   "outputs": [],
   "source": [
    "planner = MCTSPlanner(\n",
    "    net=net,\n",
    "    device=str(device),\n",
    "    step_fn=step_fn,\n",
    "    num_simulations=cfg.num_simulations,\n",
    "    cpuct=cfg.cpuct,\n",
    "    gamma=cfg.gamma,\n",
    "    pw_k=cfg.pw_k,\n",
    "    pw_alpha=cfg.pw_alpha,\n",
    "    max_depth=cfg.max_depth,\n",
    "    temperature=cfg.temperature,\n",
    "    rng=np.random.default_rng(SEED),\n",
    "    \n",
    "    K_uniform_per_node=cfg.K_uniform_per_node,\n",
    "    warmstart_iters=cfg.warmstart_iters,\n",
    "    novelty_eps=cfg.novelty_eps,\n",
    "    novelty_metric=cfg.novelty_metric,\n",
    "    num_candidates=cfg.num_candidates,\n",
    "    diversity_lambda=cfg.diversity_lambda,\n",
    "    diversity_sigma=cfg.diversity_sigma,\n",
    "    policy_beta=cfg.policy_beta,\n",
    "    max_resample_attempts=cfg.max_resample_attempts,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69aa6a37-7e5b-45e2-a028-249e5c7e924c",
   "metadata": {},
   "source": [
    "# Smoke test: one MCTS search, inspect root, pick action, step env_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "211c00be-3295-4df4-8ae3-fdb83c0e234f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root visit count N: 200\n",
      "Root children K: 29\n",
      "[0] N_sa=   2  Q_sa=-98.9491  P_raw=6.663e-02  P=0.028  action=[ 0.45089826 -0.9241004 ]\n",
      "[1] N_sa=   3  Q_sa=-66.3913  P_raw=6.322e-02  P=0.026  action=[1.         0.62617433]\n",
      "[2] N_sa=  14  Q_sa=-16.8739  P_raw=5.397e-02  P=0.022  action=[-1. -1.]\n",
      "[3] N_sa=   2  Q_sa=-99.3726  P_raw=1.131e-01  P=0.047  action=[ 0.15086427 -0.35759166]\n",
      "[4] N_sa=   4  Q_sa=-50.3960  P_raw=7.271e-02  P=0.030  action=[1.         0.49876395]\n",
      "Chosen action: [ 0.45089826 -0.9241004 ]\n",
      "Step result -> reward: -0.3392350528094017 done: False\n",
      "Obs delta L2: 1.0282367374418924\n"
     ]
    }
   ],
   "source": [
    "# Reset real env\n",
    "obs, info = env_real.reset()\n",
    "\n",
    "# Run one MCTS search from the current observation\n",
    "root = planner.search(obs)\n",
    "\n",
    "print(\"Root visit count N:\", root.N)\n",
    "print(\"Root children K:\", len(root.children))\n",
    "\n",
    "# Show a few children stats\n",
    "for i, ch in enumerate(root.children[:5]):\n",
    "    print(\n",
    "        f\"[{i}] N_sa={ch.N_sa:4d}  Q_sa={ch.Q_sa:+.4f}  \"\n",
    "        f\"P_raw={ch.P_sa_raw:.3e}  P={ch.P_sa:.3f}  action={ch.action}\"\n",
    "    )\n",
    "\n",
    "# Pick an action from MCTS policy (training=True samples from visit counts)\n",
    "action = planner.act(root, training=True)\n",
    "print(\"Chosen action:\", action)\n",
    "\n",
    "# Step the real environment once\n",
    "next_obs, reward, terminated, truncated, info = env_real.step(action)\n",
    "done = bool(terminated or truncated)\n",
    "\n",
    "print(\"Step result -> reward:\", reward, \"done:\", done)\n",
    "print(\"Obs delta L2:\", np.linalg.norm(next_obs - obs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "004c9ca5-92c2-4bb4-a2f6-7dba78b9264e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] N_sa=   2  Q_sa=-98.9491  P_raw=6.663e-02  P=0.028  action=[ 0.45089826 -0.9241004 ]\n",
      "[1] N_sa=   3  Q_sa=-66.3913  P_raw=6.322e-02  P=0.026  action=[1.         0.62617433]\n",
      "[2] N_sa=  14  Q_sa=-16.8739  P_raw=5.397e-02  P=0.022  action=[-1. -1.]\n",
      "[3] N_sa=   2  Q_sa=-99.3726  P_raw=1.131e-01  P=0.047  action=[ 0.15086427 -0.35759166]\n",
      "[4] N_sa=   4  Q_sa=-50.3960  P_raw=7.271e-02  P=0.030  action=[1.         0.49876395]\n",
      "[5] N_sa=   1  Q_sa=-199.0000  P_raw=7.428e-02  P=0.031  action=[ 0.727041 -0.782664]\n",
      "[6] N_sa=  10  Q_sa=-21.9991  P_raw=9.345e-02  P=0.039  action=[-0.37957004 -0.6430504 ]\n",
      "[7] N_sa=   2  Q_sa=-98.8708  P_raw=9.731e-02  P=0.040  action=[0.8369603  0.17599861]\n",
      "[8] N_sa=   5  Q_sa=-40.7456  P_raw=5.242e-02  P=0.022  action=[0.9788892 0.7736504]\n",
      "[9] N_sa=  11  Q_sa=-20.9593  P_raw=1.003e-01  P=0.041  action=[-1.          0.07167155]\n",
      "[10] N_sa=  25  Q_sa=-18.8314  P_raw=9.788e-02  P=0.040  action=[-1.         -0.36027542]\n",
      "[11] N_sa=   4  Q_sa=-50.7629  P_raw=9.961e-02  P=0.041  action=[0.21008626 0.34881282]\n",
      "[12] N_sa=   3  Q_sa=-67.3730  P_raw=9.349e-02  P=0.039  action=[0.17560522 0.43042472]\n",
      "[13] N_sa=  13  Q_sa=-17.9284  P_raw=1.074e-01  P=0.044  action=[-0.86223215 -0.10027298]\n",
      "[14] N_sa=  11  Q_sa=-20.8638  P_raw=4.798e-02  P=0.020  action=[-0.7074631  0.9069183]\n",
      "[15] N_sa=   3  Q_sa=-66.8346  P_raw=1.039e-01  P=0.043  action=[ 0.05636249 -0.5245132 ]\n",
      "[16] N_sa=   6  Q_sa=-34.5905  P_raw=5.821e-02  P=0.024  action=[-0.7272202 -1.       ]\n",
      "[17] N_sa=  10  Q_sa=-22.6049  P_raw=1.020e-01  P=0.042  action=[-1.         -0.22025682]\n",
      "[18] N_sa=   1  Q_sa=-199.0000  P_raw=6.545e-02  P=0.027  action=[ 1.        -0.8168191]\n",
      "[19] N_sa=  16  Q_sa=-15.3240  P_raw=5.974e-02  P=0.025  action=[-0.76304144  0.75589484]\n",
      "[20] N_sa=   5  Q_sa=-41.6859  P_raw=9.702e-02  P=0.040  action=[0.08766818 0.3939904 ]\n",
      "[21] N_sa=   3  Q_sa=-66.6521  P_raw=6.555e-02  P=0.027  action=[ 0.23352946 -0.9588576 ]\n",
      "[22] N_sa=   1  Q_sa=-199.0000  P_raw=5.155e-02  P=0.021  action=[ 1. -1.]\n",
      "[23] N_sa=   6  Q_sa=-35.2486  P_raw=9.797e-02  P=0.041  action=[-1.          0.13768315]\n",
      "[24] N_sa=   8  Q_sa=-26.6046  P_raw=1.147e-01  P=0.047  action=[-0.46775657  0.02811236]\n",
      "[25] N_sa=   1  Q_sa=-199.0000  P_raw=9.662e-02  P=0.040  action=[ 1.         -0.25928488]\n",
      "[26] N_sa=  23  Q_sa=-11.7880  P_raw=8.650e-02  P=0.036  action=[-1.          0.35247922]\n",
      "[27] N_sa=   4  Q_sa=-50.1227  P_raw=6.903e-02  P=0.029  action=[1.         0.54888195]\n",
      "[28] N_sa=   3  Q_sa=-1.5924  P_raw=1.157e-01  P=0.048  action=[-0.37066293  0.04397376]\n",
      "unique rows: 29  /  29\n"
     ]
    }
   ],
   "source": [
    "# Show all children stats\n",
    "for i, ch in enumerate(root.children):\n",
    "    print(\n",
    "        f\"[{i}] N_sa={ch.N_sa:4d}  Q_sa={ch.Q_sa:+.4f}  \"\n",
    "        f\"P_raw={ch.P_sa_raw:.3e}  P={ch.P_sa:.3f}  action={ch.action}\"\n",
    "    )\n",
    "\n",
    "actions = np.stack([ch.action for ch in root.children], axis=0)\n",
    "print(\"unique rows:\", np.unique(actions, axis=0).shape[0], \" / \", actions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690a2d1e-1166-4645-b7ed-a5d03ffad8cb",
   "metadata": {},
   "source": [
    "# create buffer, collect data, train, and log losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d10625-081b-4b0b-9cdb-653673a206a9",
   "metadata": {},
   "source": [
    "## buffer and logging containers; schedules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a19b9c0-71ea-4730-bafe-1ffbcc6a4c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buffer\n",
    "replay = ReplayBufferHybrid(\n",
    "    capacity=cfg.replay_buffer_capacity,\n",
    "    obs_dim=obs_dim,\n",
    "    action_dim=action_dim,\n",
    "    K_max=32,\n",
    ")\n",
    "\n",
    "# Logging containers (easy to plot later)\n",
    "logs = {\n",
    "    \"loss_total\": [],\n",
    "    \"loss_value\": [],\n",
    "    \"loss_policy\": [],\n",
    "    \"loss_policy_distill\": [],\n",
    "    \"loss_mu\": [],\n",
    "    \"loss_log_std\": [],\n",
    "    \"ep_return\": [],\n",
    "    \"ep_length\": [],\n",
    "    \"eval_return_mean\": [],\n",
    "    \"eval_return_std\": [],\n",
    "    \"eval_length_mean\": [],\n",
    "    \"success_rate\": [],\n",
    "    \"collision_rate\": [],\n",
    "    \"iter_idx_eval\": [],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97fc4d7e-3370-48bc-8d7b-22ea058a1e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## schedules (for later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "193277ce-b0e5-4fc9-bf89-fc3dc3f0d2f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# A simple schedule for gaussian regression weight λ\\ndef lambda_gauss(iter_idx: int) -> float:\\n    # Example: off for first 10 iters, then ramp to 1.0 over 20 iters\\n    start = 10\\n    ramp = 20\\n    if iter_idx < start:\\n        return 0.0\\n    x = min(1.0, (iter_idx - start) / float(ramp))\\n    return float(x)  # ramps 0 -> 1\\n\\n\\n# Value target mixing schedule (optional)\\ndef value_mix(iter_idx: int):\\n    # Example: start with MC a bit, then prefer MCTS\\n    # You can flip this if you want.\\n    w_mcts = 1.0\\n    w_mc = 0.0\\n    return w_mcts, w_mc\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# A simple schedule for gaussian regression weight λ\n",
    "def lambda_gauss(iter_idx: int) -> float:\n",
    "    # Example: off for first 10 iters, then ramp to 1.0 over 20 iters\n",
    "    start = 10\n",
    "    ramp = 20\n",
    "    if iter_idx < start:\n",
    "        return 0.0\n",
    "    x = min(1.0, (iter_idx - start) / float(ramp))\n",
    "    return float(x)  # ramps 0 -> 1\n",
    "\n",
    "\n",
    "# Value target mixing schedule (optional)\n",
    "def value_mix(iter_idx: int):\n",
    "    # Example: start with MC a bit, then prefer MCTS\n",
    "    # You can flip this if you want.\n",
    "    w_mcts = 1.0\n",
    "    w_mc = 0.0\n",
    "    return w_mcts, w_mc\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e293864-d5f1-4412-8b4b-0803446aa111",
   "metadata": {},
   "source": [
    "# Training (outer loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9b64688-129f-44f5-b35c-ac1321acc928",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'value_target'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 45\u001b[39m\n\u001b[32m     35\u001b[39m         loss_dict = train_step_mcts_distill(\n\u001b[32m     36\u001b[39m         net=net,\n\u001b[32m     37\u001b[39m         optimizer=optimizer,\n\u001b[32m   (...)\u001b[39m\u001b[32m     42\u001b[39m         grad_clip_norm=\u001b[32m1.0\u001b[39m,\n\u001b[32m     43\u001b[39m         )\n\u001b[32m     44\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m loss_dict.items():\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m             \u001b[43mlogs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m.append(v)\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# ---- Eval (fixed seeds) ----\u001b[39;00m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (it % cfg.eval_every) == \u001b[32m0\u001b[39m:\n",
      "\u001b[31mKeyError\u001b[39m: 'value_target'"
     ]
    }
   ],
   "source": [
    "num_iters = 30\n",
    "\n",
    "for it in range(num_iters):\n",
    "    planner.set_training_iter(it)\n",
    "\n",
    "    # ---- Collect ----\n",
    "    ep_returns = []\n",
    "    for _ in range(cfg.episodes_per_iter):\n",
    "        stats, _ = collect_one_episode_hybrid(\n",
    "            env_real=env_real,\n",
    "            planner=planner,\n",
    "            replay_buffer=replay,\n",
    "            max_steps=cfg.max_episode_steps,\n",
    "            gamma=cfg.gamma,\n",
    "            training=True,\n",
    "        )\n",
    "        ep_returns.append(stats[\"return\"])\n",
    "        logs[\"ep_return\"].append(stats[\"return\"])\n",
    "        logs[\"ep_length\"].append(stats[\"length\"])\n",
    "\n",
    "    # ---- Train (baseline MLE/value regression) ----\n",
    "    if len(replay) >= cfg.batch_size:\n",
    "        for _ in range(cfg.train_steps_per_iter):\n",
    "            batch = replay.sample(cfg.batch_size, device=device, rng=np.random.default_rng(SEED))\n",
    "            \"\"\"\n",
    "            loss_dict = train_step_mle(\n",
    "                net=net,\n",
    "                optimizer=optimizer,\n",
    "                batch=batch,\n",
    "                w_value=cfg.value_loss_weight,\n",
    "                w_policy=cfg.policy_loss_weight,\n",
    "                grad_clip_norm=1.0,\n",
    "            )\n",
    "            \"\"\"\n",
    "            loss_dict = train_step_mcts_distill(\n",
    "            net=net,\n",
    "            optimizer=optimizer,\n",
    "            batch=batch,\n",
    "            value_target=\"mc\",   # teammate’s version\n",
    "            w_value=cfg.value_loss_weight,\n",
    "            w_policy=cfg.policy_loss_weight,\n",
    "            grad_clip_norm=1.0,\n",
    "            )\n",
    "            for k, v in loss_dict.items():\n",
    "                logs[k].append(v)\n",
    "\n",
    "    # ---- Eval (fixed seeds) ----\n",
    "    if (it % cfg.eval_every) == 0:\n",
    "        eval_stats = run_eval_episodes(\n",
    "            env_eval=env_eval,\n",
    "            planner=planner,\n",
    "            seeds=EVAL_SEEDS,\n",
    "            max_steps=cfg.max_episode_steps,\n",
    "            goal_reward=env_eval._goal_reward,\n",
    "            collision_reward=env_eval._collision_reward,\n",
    "        )\n",
    "        logs[\"iter_idx_eval\"].append(it)\n",
    "        logs[\"eval_return_mean\"].append(eval_stats[\"eval_return_mean\"])\n",
    "        logs[\"eval_return_std\"].append(eval_stats[\"eval_return_std\"])\n",
    "        logs[\"eval_length_mean\"].append(eval_stats[\"eval_length_mean\"])\n",
    "        logs[\"success_rate\"].append(eval_stats[\"success_rate\"])\n",
    "        logs[\"collision_rate\"].append(eval_stats[\"collision_rate\"])\n",
    "\n",
    "        print(\n",
    "            f\"[Eval it={it}] \"\n",
    "            f\"R={eval_stats['eval_return_mean']:.2f}±{eval_stats['eval_return_std']:.2f} \"\n",
    "            f\"len={eval_stats['eval_length_mean']:.1f} \"\n",
    "            f\"succ={eval_stats['success_rate']:.2f} \"\n",
    "            f\"coll={eval_stats['collision_rate']:.2f}\"\n",
    "        )\n",
    "\n",
    "    last_loss = logs[\"loss_total\"][-1] if logs[\"loss_total\"] else None\n",
    "    print(\n",
    "        f\"Iter {it} | replay={len(replay)} | \"\n",
    "        f\"train_return_mean={np.mean(ep_returns):.2f} | last_loss={last_loss}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100cd18c-41fb-473c-8aae-fcb8584dfb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "num_iters = 30  # start with 30; increase later\n",
    "\n",
    "# Add eval logs\n",
    "logs.setdefault(\"eval_return_mean\", [])\n",
    "logs.setdefault(\"eval_return_std\", [])\n",
    "logs.setdefault(\"eval_length_mean\", [])\n",
    "logs.setdefault(\"success_rate\", [])\n",
    "logs.setdefault(\"collision_rate\", [])\n",
    "logs.setdefault(\"iter_idx_eval\", [])\n",
    "\n",
    "# Simple schedules: start clean\n",
    "def lambda_gauss(iter_idx: int) -> float:\n",
    "    return 0.0  # baseline: off\n",
    "\n",
    "def value_mix(iter_idx: int):\n",
    "    return 1.0, 0.0  # baseline: use z_mcts only\n",
    "\n",
    "\n",
    "for it in range(num_iters):\n",
    "    # tell planner what outer iteration we are in\n",
    "    planner.set_training_iter(it)\n",
    "\n",
    "    # --- Collect episodes ---\n",
    "    ep_returns = []\n",
    "    ep_lengths = []\n",
    "    for _ in range(cfg.episodes_per_iter):\n",
    "        stats, _ = collect_one_episode_hybrid(\n",
    "            env_real=env_real,\n",
    "            planner=planner,\n",
    "            replay_buffer=replay,\n",
    "            max_steps=cfg.max_episode_steps,\n",
    "            gamma=cfg.gamma,\n",
    "            training=True,\n",
    "        )\n",
    "        ep_returns.append(stats[\"return\"])\n",
    "        ep_lengths.append(stats[\"length\"])\n",
    "        logs[\"ep_return\"].append(stats[\"return\"])\n",
    "        logs[\"ep_length\"].append(stats[\"length\"])\n",
    "\n",
    "    # --- Train ---\n",
    "    if len(replay) >= cfg.batch_size:\n",
    "        w_mcts, w_mc = value_mix(it)\n",
    "        w_gauss = lambda_gauss(it)\n",
    "\n",
    "        for _ in range(cfg.train_steps_per_iter):\n",
    "            batch = replay.sample(cfg.batch_size, device=device, rng=np.random.default_rng(SEED))\n",
    "            loss_dict = train_step_hybrid(\n",
    "                net=net,\n",
    "                optimizer=optimizer,\n",
    "                batch=batch,\n",
    "                w_value_mcts=w_mcts * cfg.value_loss_weight,\n",
    "                w_value_mc=w_mc * cfg.value_loss_weight,\n",
    "                w_policy_imitation=cfg.policy_loss_weight,\n",
    "                w_gaussian_reg=w_gauss * cfg.policy_loss_weight,\n",
    "                grad_clip_norm=1.0,\n",
    "            )\n",
    "            for k, v in loss_dict.items():\n",
    "                logs[k].append(v)\n",
    "\n",
    "    # --- Eval (fixed seeds) ---\n",
    "    if (it % cfg.eval_every) == 0:\n",
    "        eval_stats = run_eval_episodes(\n",
    "            env_eval=env_eval,\n",
    "            planner=planner,\n",
    "            seeds=EVAL_SEEDS,\n",
    "            max_steps=cfg.max_episode_steps,\n",
    "            goal_reward=env_eval._goal_reward,\n",
    "            collision_reward=env_eval._collision_reward,\n",
    "        )\n",
    "        logs[\"iter_idx_eval\"].append(it)\n",
    "        logs[\"eval_return_mean\"].append(eval_stats[\"eval_return_mean\"])\n",
    "        logs[\"eval_return_std\"].append(eval_stats[\"eval_return_std\"])\n",
    "        logs[\"eval_length_mean\"].append(eval_stats[\"eval_length_mean\"])\n",
    "        logs[\"success_rate\"].append(eval_stats[\"success_rate\"])\n",
    "        logs[\"collision_rate\"].append(eval_stats[\"collision_rate\"])\n",
    "\n",
    "        print(\n",
    "            f\"[Eval it={it}] \"\n",
    "            f\"R={eval_stats['eval_return_mean']:.2f}±{eval_stats['eval_return_std']:.2f} \"\n",
    "            f\"len={eval_stats['eval_length_mean']:.1f} \"\n",
    "            f\"succ={eval_stats['success_rate']:.2f} \"\n",
    "            f\"coll={eval_stats['collision_rate']:.2f}\"\n",
    "        )\n",
    "\n",
    "    last_loss = logs[\"loss_total\"][-1] if logs[\"loss_total\"] else None\n",
    "    print(\n",
    "        f\"Iter {it} | replay={len(replay)} | \"\n",
    "        f\"train_return_mean={np.mean(ep_returns):.2f} | \"\n",
    "        f\"last_loss={last_loss}\"\n",
    "    )\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd07941-56e3-4c05-bf1f-c75c6a373b63",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a05d54-32c2-4e43-8c9a-4c81457032d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_avg(x, w=20):\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    if len(x) < w:\n",
    "        return x\n",
    "    return np.convolve(x, np.ones(w)/w, mode=\"valid\")\n",
    "\n",
    "# Train return moving average\n",
    "plt.figure()\n",
    "plt.plot(moving_avg(logs[\"ep_return\"], w=20))\n",
    "plt.title(\"Train episode return (moving avg)\")\n",
    "plt.xlabel(\"episodes\")\n",
    "plt.ylabel(\"return\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Eval return mean\n",
    "if logs[\"eval_return_mean\"]:\n",
    "    plt.figure()\n",
    "    plt.plot(logs[\"iter_idx_eval\"], logs[\"eval_return_mean\"], marker=\"o\")\n",
    "    plt.title(\"Eval return mean (fixed seeds)\")\n",
    "    plt.xlabel(\"iteration\")\n",
    "    plt.ylabel(\"return\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Eval success rate\n",
    "if logs[\"success_rate\"]:\n",
    "    plt.figure()\n",
    "    plt.plot(logs[\"iter_idx_eval\"], logs[\"success_rate\"], marker=\"o\")\n",
    "    plt.title(\"Eval success rate (fixed seeds)\")\n",
    "    plt.xlabel(\"iteration\")\n",
    "    plt.ylabel(\"success rate\")\n",
    "    plt.ylim(-0.05, 1.05)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Losses\n",
    "if logs[\"loss_total\"]:\n",
    "    plt.figure()\n",
    "    plt.plot(logs[\"loss_total\"])\n",
    "    plt.title(\"Total loss\")\n",
    "    plt.xlabel(\"train steps\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(logs[\"loss_value\"])\n",
    "    plt.title(\"Value loss\")\n",
    "    plt.xlabel(\"train steps\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(logs[\"loss_policy\"])\n",
    "    plt.title(\"Policy loss (mu/log_std regression)\")\n",
    "    plt.xlabel(\"train steps\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
